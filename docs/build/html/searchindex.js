Search.setIndex({docnames:["api/bellman","api/index","api/mdp","api/solvers","api/utils","index"],envversion:{"sphinx.domains.c":1,"sphinx.domains.changeset":1,"sphinx.domains.cpp":1,"sphinx.domains.javascript":1,"sphinx.domains.math":1,"sphinx.domains.python":1,"sphinx.domains.rst":1,"sphinx.domains.std":1,"sphinx.ext.intersphinx":1,"sphinx.ext.viewcode":1,sphinx:55},filenames:["api/bellman.rst","api/index.rst","api/mdp.rst","api/solvers.rst","api/utils.rst","index.rst"],objects:{"pymdp.bellman":{bellman_difference:[0,1,1,""],bellman_operator:[0,1,1,""],bellman_step:[0,1,1,""],generate_policy_from:[0,1,1,""]},"pymdp.mdp":{MDPFunction:[2,2,1,""],MarkovDecisionProcess:[2,2,1,""],default_policy:[2,1,1,""],default_value:[2,1,1,""]},"pymdp.mdp.MarkovDecisionProcess":{actions:[2,3,1,""],discount:[2,3,1,""],reward:[2,3,1,""],states:[2,3,1,""],transition:[2,3,1,""]},"pymdp.solvers":{linear:[3,0,0,"-"],policy:[3,0,0,"-"],value:[3,0,0,"-"]},"pymdp.solvers.linear":{linear_programming:[3,1,1,""]},"pymdp.solvers.policy":{can_improve:[3,1,1,""],determine_improvements:[3,1,1,""],improve_policy:[3,1,1,""],policy_iteration:[3,1,1,""],policy_reward:[3,1,1,""],policy_transition:[3,1,1,""],solve_for_value:[3,1,1,""]},"pymdp.solvers.value":{value_iteration:[3,1,1,""]},"pymdp.utils":{sup_distance:[4,1,1,""],sup_norm:[4,1,1,""]},pymdp:{bellman:[0,0,0,"-"],mdp:[2,0,0,"-"],utils:[4,0,0,"-"]}},objnames:{"0":["py","module","Python module"],"1":["py","function","Python function"],"2":["py","class","Python class"],"3":["py","attribute","Python attribute"]},objtypes:{"0":"py:module","1":"py:function","2":"py:class","3":"py:attribute"},terms:{"boolean":3,"class":[2,4],"default":2,"float":[0,2,4],"function":[0,2,3,4],"int":2,"public":1,"return":[0,2,3,4],For:[0,3],The:[0,2,3],__call__:2,_policy_iter:[],_value_iter:[],access:2,achiev:0,act:2,action:[0,2,3],actual:[2,3],adopt:3,algorithm:[1,5],alia:2,all:3,along:2,ani:[2,3],arbitrari:2,arriv:[2,3],assign:2,associ:[0,3],assum:4,avail:2,becaus:2,bellman:[1,5],bellman_differ:0,bellman_oper:0,bellman_step:0,beta:[0,2],between:2,both:0,bound:[0,4],call:2,can:[2,3,4],can_improv:3,cdot:0,check:3,collect:4,combin:3,compon:0,comput:[0,3,4],consid:0,constant:2,contain:[1,4],correspond:[0,3],creat:0,decis:[0,1,5],default_polici:2,default_valu:2,defin:[0,2,4],denot:0,determine_improv:3,dict:[0,2],dictionari:[],discount:[0,2],distanc:4,document:1,each:[2,3],entri:3,epsilon:3,error:3,essenti:[0,2],eta:3,everi:2,evolv:2,face:2,factor:[0,2],field:2,finit:4,formal:2,from:[2,3],full:1,fun1:4,fun2:4,fun:4,gener:2,generate_policy_from:0,get:3,given:[0,2,3,4],have:4,horizon:3,implement:[2,3],improv:3,improve_polici:3,increas:3,index:[2,5],indic:2,infinit:3,infti:4,instrument:0,interfac:2,intern:2,item:2,iter:[0,3],its:2,just:3,librari:[2,4],linear:3,linear_program:1,made:3,mai:2,mani:2,markov:[0,1,5],markovdecisionprocess:[0,2,3],masquerad:2,mathbb:[0,2,4],matrix:3,max_:[0,4],maximum:4,mdp:[0,1,4,5],mdpfunction:2,mid:3,modifi:3,modul:5,need:0,norm:4,note:2,number:2,numer:[0,2,3],object:[0,2],one:[0,2],onli:3,oper:[1,4,5],origin:2,output:2,over:2,page:5,paramet:[0,2,4],part:1,particular:[3,4],perform:[0,3],phi:3,point:[2,4],polici:[0,1,2],policy_iter:3,policy_reward:3,policy_transit:3,possibl:3,practic:0,present:3,probabl:[0,2,3],problem:3,process:[0,1,5],program:3,promis:2,provid:2,pymdp:1,python:2,relat:[1,5],replac:4,repres:2,represent:2,requir:2,requisit:2,result:0,reward:[0,2],rtype:[],search:5,set:[0,2,3],should:2,sinc:[2,4],skip:3,solut:3,solv:3,solve_for_valu:3,solver:[1,5],some:2,sourc:[0,2,3,4],space:[0,2,4],specif:0,specifi:0,standard:2,state:[0,2,3,4],stationari:[],step:2,strictli:3,sum_:[0,3],sup:4,sup_:4,sup_dist:4,sup_norm:4,suppli:2,supremum:4,system:3,term:3,thei:2,them:3,theoret:0,thi:[0,1,2],those:3,thought:2,through:3,throughout:[2,4],thu:3,time:2,todo:2,tool:[1,5],transit:[0,2,3],tupl:2,two:4,type:[0,2,4],unset:2,user:2,uses:4,usual:2,util:[1,5],valu:[0,1,2,4],value_iter:3,via:2,were:3,where:[0,2],whether:3,which:[0,2,3,4],whose:2,would:3,yield:[],zero:[]},titles:["Bellman operator and related tools","API Reference","Markov Decision Processes","MDP solver algorithms","Utilities","Welcome to PyMDP\u2019s documentation!"],titleterms:{_value_iter:[],algorithm:3,api:[1,5],bellman:0,decis:2,document:5,indic:5,linear_program:3,markov:2,mdp:[2,3],oper:0,polici:3,process:2,pymdp:[0,2,3,4,5],refer:[1,5],relat:0,solver:3,tabl:5,tool:0,util:4,valu:3,welcom:5}})