Search.setIndex({docnames:["api/bellman","api/index","api/mdp","api/solvers","api/utils","index"],envversion:{"sphinx.domains.c":1,"sphinx.domains.changeset":1,"sphinx.domains.cpp":1,"sphinx.domains.javascript":1,"sphinx.domains.math":1,"sphinx.domains.python":1,"sphinx.domains.rst":1,"sphinx.domains.std":1,"sphinx.ext.intersphinx":1,"sphinx.ext.viewcode":1,sphinx:55},filenames:["api/bellman.rst","api/index.rst","api/mdp.rst","api/solvers.rst","api/utils.rst","index.rst"],objects:{"pymdp._policy_iteration":{can_improve:[3,1,1,""],determine_improvements:[3,1,1,""],improve_policy:[3,1,1,""],policy_iteration:[3,1,1,""],policy_reward:[3,1,1,""],policy_transition:[3,1,1,""],solve_for_value:[3,1,1,""]},"pymdp._value_iteration":{value_iteration:[3,1,1,""]},"pymdp.bellman":{bellman_difference:[0,1,1,""],bellman_operator:[0,1,1,""],bellman_step:[0,1,1,""]},"pymdp.mdp":{MarkovDecisionProcess:[2,2,1,""],default_policy:[2,1,1,""],default_value:[2,1,1,""]},"pymdp.mdp.MarkovDecisionProcess":{actions:[2,3,1,""],discount:[2,3,1,""],reward:[2,3,1,""],states:[2,3,1,""],transition:[2,3,1,""]},"pymdp.utils":{sup_distance:[4,1,1,""],sup_norm:[4,1,1,""]},pymdp:{_policy_iteration:[3,0,0,"-"],_value_iteration:[3,0,0,"-"],bellman:[0,0,0,"-"],mdp:[2,0,0,"-"],utils:[4,0,0,"-"]}},objnames:{"0":["py","module","Python module"],"1":["py","function","Python function"],"2":["py","class","Python class"],"3":["py","attribute","Python attribute"]},objtypes:{"0":"py:module","1":"py:function","2":"py:class","3":"py:attribute"},terms:{"boolean":3,"class":[2,4],"default":2,"float":[0,4],"function":[0,2,3,4],"public":1,"return":[0,2,3,4],For:[0,3],The:[0,3],_policy_iter:3,_value_iter:1,achiev:0,action:[0,2,3],actual:3,adopt:3,algorithm:[1,5],alia:2,all:3,ani:3,arbitrari:2,arriv:3,assign:2,associ:[0,2,3],assum:4,avail:2,bellman:[1,5],bellman_differ:0,bellman_oper:0,bellman_step:0,beta:0,both:0,bound:[0,4],can:[3,4],can_improv:3,cdot:0,check:3,collect:4,combin:3,compon:0,comput:[0,3,4],consid:0,contain:[1,4],correspond:[0,3],decis:[0,1,5],default_polici:2,default_valu:2,defin:[0,4],denot:0,determine_improv:3,dict:0,dictionari:2,discount:[0,2],distanc:4,document:1,each:[2,3],entri:3,epsilon:3,error:3,essenti:0,eta:3,everi:2,factor:0,field:2,finit:4,from:3,full:1,fun1:4,fun2:4,fun:4,get:3,given:[0,3,4],have:4,horizon:3,implement:[2,3],improv:3,improve_polici:3,increas:3,index:5,infinit:3,infti:4,instrument:0,iter:[0,3],just:3,librari:4,made:3,markov:[0,1,5],markovdecisionprocess:[0,2,3],mathbb:[0,4],matrix:3,max_:[0,4],maximum:4,mdp:[0,1,2,4,5],mid:3,modifi:3,modul:5,need:0,norm:4,note:2,number:2,numer:[0,3],object:0,one:0,onli:3,oper:[1,4,5],page:5,paramet:[0,4],part:1,particular:[3,4],perform:[0,3],phi:3,point:4,polici:[0,2,3],policy_iter:3,policy_reward:3,policy_transit:3,possibl:3,practic:0,present:3,probabl:[0,3],problem:3,process:[0,1,5],pymdp:[1,2],relat:[1,5],replac:4,requir:2,result:0,reward:[0,2],search:5,set:[0,3],sinc:4,skip:3,solv:3,solve_for_valu:3,solver:[1,5],some:2,sourc:[0,2,3,4],space:[0,4],specif:0,specifi:0,state:[0,2,3,4],stationari:2,strictli:3,sum_:[0,3],sup:4,sup_:4,sup_dist:4,sup_norm:4,supremum:4,system:3,term:3,them:3,theoret:0,thi:[0,1],those:3,through:3,throughout:4,thu:3,tool:[1,5],transit:[0,2,3],two:4,type:[0,4],uses:4,util:[1,5],valu:[0,2,3,4],value_iter:3,were:3,where:0,whether:3,which:[0,2,3,4],would:3,yield:2,zero:2},titles:["Bellman operator and related tools","API Reference","Markov Decision Processes","MDP solver algorithms","Utilities","Welcome to PyMDP\u2019s documentation!"],titleterms:{_value_iter:3,algorithm:3,api:[1,5],bellman:0,decis:2,document:5,indic:5,markov:2,mdp:3,oper:0,process:2,pymdp:[0,3,4,5],refer:[1,5],relat:0,solver:3,tabl:5,tool:0,util:4,welcom:5}})